{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AAAI ICWSM 2024 | Tutorial | June 3rd | 2-6pm\n",
        "## <u>Collectivist and Perspectivist Approaches to Studying Online Toxicity</u>\n",
        "### Yotam Shmargad, School of Government & Public Policy, University of Arizona"
      ],
      "metadata": {
        "id": "7p6JD3Qb6kRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This is the <b>first</b> of two notebooks we will discuss during the tutorial. The notebook is written in Python and walks participants through analyzing YouTube comments from a *Collectivist* approach. The notebook:\n",
        "1.   Collects YouTube videos for <b>two</b> separate queries\n",
        "2.   Collects a set of comments for the videos from each query\n",
        "3.   Analyzes *hatefulness* in the two sets of comments\n",
        "4.   Compares the average hatefulness in the two sets *(descriptive norm)*\n",
        "5.   Compares the association between hatefulness and like counts in the two sets *(injunctive norm)*\n"
      ],
      "metadata": {
        "id": "tm0HILc90vbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Collect YouTube videos for <b>two</b> separate queries`"
      ],
      "metadata": {
        "id": "aubqPGQYYcoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWZkSSMdz4SP"
      },
      "outputs": [],
      "source": [
        "# Install library for YouTube data collection https://youtube-data-api.readthedocs.io/en/latest/youtube_api.html\n",
        "!pip install youtube-data-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries for data collection, management, and analysis\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from youtube_api import YouTubeDataAPI\n",
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "yHcr6JOS9bUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain API Key from https://console.cloud.google.com/apis/\n",
        "# Authenticate with YouTube - place the API Key you obtained between the quotes\n",
        "yt = YouTubeDataAPI(\"\")"
      ],
      "metadata": {
        "id": "NOQBvbES9vmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect 50 videos using a query search - place your search term between the quotes\n",
        "vids1 = yt.search(\"trump\",max_results = 50)"
      ],
      "metadata": {
        "id": "YPIObImvAhpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect 50 videos using a different query\n",
        "vids2 = yt.search(\"biden\",max_results = 50)"
      ],
      "metadata": {
        "id": "_-wEZ-cNBuFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of videos collected for each query\n",
        "print(\"Query 1:\",len(vids1))\n",
        "print(\"Query 2:\",len(vids2))"
      ],
      "metadata": {
        "id": "24c4YFpUB3Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve information for the first video in the first query\n",
        "vids1[0]"
      ],
      "metadata": {
        "id": "DMq9ZsbjCArr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Video ID information for the first video in the first query\n",
        "vids1[0]['video_id']"
      ],
      "metadata": {
        "id": "753lw2fpCZX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Place video data into pandas tables\n",
        "vids1_df = pd.DataFrame(vids1)\n",
        "vids2_df = pd.DataFrame(vids2)"
      ],
      "metadata": {
        "id": "W_3JvKwjM4d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print table for videos from first query\n",
        "vids1_df"
      ],
      "metadata": {
        "id": "KilGBn_2NC2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Collect a set of comments for the videos from each query"
      ],
      "metadata": {
        "id": "1hCuDUwZRVRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve 20 comments from the first video for first query\n",
        "com = yt.get_video_comments(vids1[0]['video_id'],get_replies = False,max_results = 20)"
      ],
      "metadata": {
        "id": "HQxy2NEQNIqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of comments collected for first video of first query\n",
        "len(com)"
      ],
      "metadata": {
        "id": "-9fsVXGVNTW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve information for the first comment for first video in first query\n",
        "com[0]"
      ],
      "metadata": {
        "id": "RHP2SFqgOBRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Place comment data into a pandas table\n",
        "com_df = pd.DataFrame(com)"
      ],
      "metadata": {
        "id": "_eZSb2F6OIw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract three columns from pandas table\n",
        "small_com_df = com_df[['video_id','comment_like_count','text']]"
      ],
      "metadata": {
        "id": "TTrSObuPOQMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print table for comments from first video of first query\n",
        "small_com_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w0WWf-VzOf4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of rows and columns of pandas table\n",
        "small_com_df.shape"
      ],
      "metadata": {
        "id": "Dqg-JoQAPodE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append the table to itself to test 'concat' function - this should duplicate each row\n",
        "pd.concat([small_com_df,small_com_df])"
      ],
      "metadata": {
        "id": "Jl-dUBPYOnil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define empty table for storing comments from videos in first query\n",
        "comments1 = pd.DataFrame(columns=['video_id','comment_like_count', 'text'])"
      ],
      "metadata": {
        "id": "t9YU5UVEO7uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print empty table\n",
        "comments1"
      ],
      "metadata": {
        "id": "JrRiyU28PHMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test concat function with the empty table\n",
        "pd.concat([comments1,small_com_df],ignore_index = True)"
      ],
      "metadata": {
        "id": "NjxkaqTUPJ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through videos from first query, pull 20 comments per video, and append them to a single table\n",
        "for i in vids1:\n",
        "  try:\n",
        "    c = yt.get_video_comments(i['video_id'],get_replies = False,max_results = 20)\n",
        "    cdf = pd.DataFrame(c)\n",
        "    if cdf.shape[0] > 0:\n",
        "      scdf = cdf[['video_id','comment_like_count','text']]\n",
        "      comments1 = pd.concat([comments1,scdf],ignore_index=True)\n",
        "    print(i['video_id'],'success!',cdf.shape[0])\n",
        "  except:\n",
        "    print(i['video_id'],'fail')"
      ],
      "metadata": {
        "id": "GmMDjfFSPNoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define empty table for storing comments from videos in second query\n",
        "comments2 = pd.DataFrame(columns=['video_id','comment_like_count', 'text'])"
      ],
      "metadata": {
        "id": "fs3zKSTAP7zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through videos from second query, pull 20 comments per video, and append them to a single table\n",
        "for i in vids2:\n",
        "  try:\n",
        "    c = yt.get_video_comments(i['video_id'],get_replies = False,max_results = 20)\n",
        "    cdf = pd.DataFrame(c)\n",
        "    if cdf.shape[0] > 0:\n",
        "      scdf = cdf[['video_id','comment_like_count','text']]\n",
        "      comments2 = pd.concat([comments2,scdf],ignore_index=True)\n",
        "    print(i['video_id'],'success!',cdf.shape[0])\n",
        "  except:\n",
        "    print(i['video_id'],'fail')"
      ],
      "metadata": {
        "id": "Ar6lyFakQK0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of rows and columns of two tables of comments\n",
        "print(\"Query 1:\",comments1.shape)\n",
        "print(\"Query 2:\",comments2.shape)"
      ],
      "metadata": {
        "id": "EtrsGZlGQVUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print text of first comment from first query\n",
        "comments1.text[0]"
      ],
      "metadata": {
        "id": "Q500OJNBQrCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Analyze *hatefulness* in the two sets of comments"
      ],
      "metadata": {
        "id": "jA2X1PQnRk9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install library for hatefulness analysis https://github.com/pysentimiento/pysentimiento\n",
        "!pip install pysentimiento"
      ],
      "metadata": {
        "id": "qun_3nTsQzK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library for text analysis\n",
        "from pysentimiento import create_analyzer"
      ],
      "metadata": {
        "id": "DiBeoimiRug4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load hatefulness analyzer in English\n",
        "hate = create_analyzer(task=\"hate_speech\",lang=\"en\")"
      ],
      "metadata": {
        "id": "tMYs2c8hSMUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test hatefulness analyzer on text of first comment\n",
        "hate.predict(comments1.text[0])"
      ],
      "metadata": {
        "id": "Ela-0c3ZSklT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results of hatefulness analysis of first comment\n",
        "h = hate.predict(comments1.text[0])"
      ],
      "metadata": {
        "id": "7MrsFLhSTAxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print probabilities from hatefulness analysis of first comment\n",
        "h.probas"
      ],
      "metadata": {
        "id": "Eqb8GwdLTUa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print probability of hate in first comment\n",
        "h.probas['hateful']"
      ],
      "metadata": {
        "id": "QJ7I48XITX30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty columns to store probability of hate for all comments\n",
        "comments1['hateful'] = ''\n",
        "comments2['hateful'] = ''"
      ],
      "metadata": {
        "id": "YWZo8Y6NTnnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print table of comments from first query with added column\n",
        "comments1"
      ],
      "metadata": {
        "id": "vBNpXlK0T5QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through comments of first query, analyze for hatefulness, then place scores in the table\n",
        "# NOTE: this may take several minutes to complete\n",
        "for index,row in comments1.iterrows():\n",
        "  h = hate.predict(row['text'])\n",
        "  comments1.at[index, 'hateful'] = h.probas['hateful']\n",
        "  if((index + 1) % 50 == 0):\n",
        "    print(index + 1,end=\" \")"
      ],
      "metadata": {
        "id": "SbYRlbEmT__z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print comments and hatefulness probabilities for first query\n",
        "comments1"
      ],
      "metadata": {
        "id": "DBhEem55W9WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through comments of second query, analyze for hatefulness, then place scores in the table\n",
        "# NOTE: this may take several minutes to complete\n",
        "for index,row in comments2.iterrows():\n",
        "  h = hate.predict(row['text'])\n",
        "  comments2.at[index, 'hateful'] = h.probas['hateful']\n",
        "  if((index + 1) % 50 == 0):\n",
        "    print(index + 1,end=\" \")"
      ],
      "metadata": {
        "id": "FgGuKZzoWb_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print comments and hatefulness probabilities for second query\n",
        "comments2"
      ],
      "metadata": {
        "id": "CZ0bOahDXUf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Compare the average hatefulness in the two sets *(descriptive norm)*"
      ],
      "metadata": {
        "id": "QqGq7Pj6YXE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create columns to store indicator for first/second query\n",
        "comments1['query'] = 0\n",
        "comments2['query'] = 1"
      ],
      "metadata": {
        "id": "ph26i4TfYu80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append the two tables together\n",
        "all = pd.concat([comments1,comments2],ignore_index = True)"
      ],
      "metadata": {
        "id": "uMbNBYbSY-mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print table of comments\n",
        "all"
      ],
      "metadata": {
        "id": "q81FM_OlZOco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print variable types\n",
        "all.dtypes"
      ],
      "metadata": {
        "id": "MDiRKF8idfE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert variable types to numbers\n",
        "all['comment_like_count'] = all['comment_like_count'].astype(int)\n",
        "all['hateful'] = all['hateful'].astype(float)"
      ],
      "metadata": {
        "id": "01vA2fdodrQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print variable types after changes\n",
        "all.dtypes"
      ],
      "metadata": {
        "id": "d94iAFrVd1P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create binary variable capturing if probability of hatefulness > 50%\n",
        "all['hate_thresh'] = all['hateful'].apply(lambda x: 1 if x > .5 else 0)"
      ],
      "metadata": {
        "id": "MxL4jmmBZXJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of columns where probability of hatefulness > 50%\n",
        "all.loc[all['hate_thresh'] == 1].size"
      ],
      "metadata": {
        "id": "kzghNjygeZmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create column of all 1s to add a constant to the models\n",
        "all['constant'] = 1"
      ],
      "metadata": {
        "id": "vqbL0R_pgLOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print table with all of the created variables\n",
        "all"
      ],
      "metadata": {
        "id": "j7FF4QxugQIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model the difference in hate probability across the two queries using OLS regression\n",
        "model1 = sm.OLS(all['hateful'],all[['query','constant']])"
      ],
      "metadata": {
        "id": "lpriqjGGYGdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model 1 results\n",
        "model1.fit().summary()"
      ],
      "metadata": {
        "id": "pdBph6YAgvW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model the difference in hate prevalence across the two queries using OLS regression\n",
        "model2 = sm.OLS(all['hate_thresh'],all[['query','constant']])"
      ],
      "metadata": {
        "id": "CJxs5lBAg0Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model 2 results\n",
        "model2.fit().summary()"
      ],
      "metadata": {
        "id": "LWeyXh58hD7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model the difference in hate prevalence across the two queries using Logit regression\n",
        "model3 = sm.GLM(all['hate_thresh'],all[['query','constant']],family=sm.families.Binomial())"
      ],
      "metadata": {
        "id": "tMYvj9IOhFw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model 3 results\n",
        "model3.fit().summary()"
      ],
      "metadata": {
        "id": "KGYMOkW_hqfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Compare association between hatefulness and likes in the two sets *(injunctive norm)*"
      ],
      "metadata": {
        "id": "ktOL7s5dh32l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create columns capturing the interaction between query and hatefulness\n",
        "all['interaction'] = all['query']*all['hateful']\n",
        "all['int_thresh'] = all['query']*all['hate_thresh']"
      ],
      "metadata": {
        "id": "Eo6vxgi0hwVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print updated table with all variables\n",
        "all"
      ],
      "metadata": {
        "id": "xYfYdVBki9X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model associating hate probability and likes across the two queries using OLS\n",
        "model4 = sm.OLS(all['comment_like_count'],all[['query','hateful','interaction','constant']])"
      ],
      "metadata": {
        "id": "lFJU2KphjCpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model 4 results\n",
        "model4.fit().summary()"
      ],
      "metadata": {
        "id": "rOKmWrYMjgVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model associating hate prevalence and likes across the two queries using OLS\n",
        "model5 = sm.OLS(all['comment_like_count'],all[['query','hate_thresh','int_thresh','constant']])"
      ],
      "metadata": {
        "id": "d3VPz__1jj6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model 5 results\n",
        "model5.fit().summary()"
      ],
      "metadata": {
        "id": "LP7xpDBzj2Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model associating hate probability and likes across the two queries using Negative Binomial\n",
        "model6 = sm.GLM(all['comment_like_count'],all[['query','hateful','interaction','constant']],family=sm.families.NegativeBinomial())"
      ],
      "metadata": {
        "id": "w7mseAzPj7P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model 6 results\n",
        "model6.fit().summary()"
      ],
      "metadata": {
        "id": "DwujD0s0kLaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model associating hate prevalence and likes across the two queries using Negative Binomial\n",
        "model7 = sm.GLM(all['comment_like_count'],all[['query','hate_thresh','int_thresh','constant']],family=sm.families.NegativeBinomial())"
      ],
      "metadata": {
        "id": "XHrZinQmkVdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model 7 results\n",
        "model7.fit().summary()"
      ],
      "metadata": {
        "id": "uw6iCqCVkvhM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}